# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ywfKkLGwm0l3cfLaOALt8TV7u2VCScKY
"""

!pip install transformers torch pandas numpy datasets

import torch
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset

# Load pre-trained BERT model & tokenizer
model_name = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

device = "cuda" if torch.cuda.is_available() else "cpu"

from collections import Counter

# Load dataset
file_path = "ml_insurance_challenge_annotated_cosine.csv"
stats_df = pd.read_csv(file_path)

# Convert string representation of lists to actual lists
stats_df['Predicted_Taxonomy_Labels'] = stats_df['Predicted_Taxonomy_Labels'].apply(lambda x: eval(x) if isinstance(x, str) and x.startswith('[') else [])

# Get labeled data
labeled_data = stats_df[stats_df['Predicted_Taxonomy_Labels'].apply(lambda x: len(x) > 0)]

# Count unique labels
all_labels = [label for labels in labeled_data["Predicted_Taxonomy_Labels"] for label in labels]
label_counts = Counter(all_labels)

# Print dataset stats
print(f"Total Entries in Dataset: {len(stats_df)}")
print(f"Labeled Entries: {len(labeled_data)}")
print(f"Unlabeled Entries: {len(stats_df) - len(labeled_data)}")
print(f"Unique Labels: {len(label_counts)}")
print("\nTop 10 Most Frequent Labels:")
for label, count in label_counts.most_common(10):
    print(f"   - {label}: {count} occurrences")

print(all_labels)

# Load and preprocess data
def load_and_preprocess_data(file_path):
    df = pd.read_csv(file_path)

    # Convert labels from string to list
    df["Predicted_Taxonomy_Labels"] = df["Predicted_Taxonomy_Labels"].apply(
        lambda x: eval(x) if isinstance(x, str) and x.startswith("[") else []
    )

    # Split labeled and unlabeled data
    labeled_data = df[df["Predicted_Taxonomy_Labels"].apply(len) > 0]
    unlabeled_data = df[df["Predicted_Taxonomy_Labels"].apply(len) == 0]

    # Fit MultiLabelBinarizer on ALL labels in dataset (to prevent missing classes)
    mlb = MultiLabelBinarizer()
    y = mlb.fit_transform(df["Predicted_Taxonomy_Labels"])

    return df, labeled_data, unlabeled_data, mlb, y

# Load dataset
file_path = "ml_insurance_challenge_annotated_cosine.csv"
df, labeled_data, unlabeled_data, mlb, y = load_and_preprocess_data(file_path)

# Train-validation split
train_texts, val_texts, train_labels, val_labels = train_test_split(
    labeled_data["combined_text"].tolist(),
    y[: len(labeled_data)],  # Ensure shape matches labeled data
    test_size=0.2,
    random_state=42
)

# Preprocessing function for tokenization
def preprocess_function(texts, labels=None):
    encodings = tokenizer(texts, padding="max_length", truncation=True, max_length=512)
    dataset_dict = {
        "input_ids": encodings["input_ids"],
        "attention_mask": encodings["attention_mask"]
    }
    if labels is not None:
        dataset_dict["labels"] = torch.tensor(labels, dtype=torch.float)  # Multi-label format

    return Dataset.from_dict(dataset_dict)

# Preprocess datasets
train_dataset = preprocess_function(train_texts, train_labels)
val_dataset = preprocess_function(val_texts, val_labels)

num_labels = len(mlb.classes_)
model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=num_labels,
    problem_type="multi_label_classification"
)
model.to(device)

#print(df["Predicted_Taxonomy_Labels"])
#print(mlb.classes_)
#print(len(mlb.classes_))
print(train_labels)

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=5,
    logging_dir="./logs",
    logging_steps=10,
    save_total_limit=2,
    load_best_model_at_end=True,
)

# Trainer setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer
)

# Train the model
trainer.train()

torch.cuda.empty_cache()

# Batch-based prediction function to prevent CUDA Out of Memory
def predict_unlabeled_data(texts, mlb, batch_size=16, threshold=0.01):
    all_predictions = []

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i : i + batch_size]

        encodings = tokenizer(batch_texts, padding=True, truncation=True, max_length=512, return_tensors="pt")
        encodings = {k: v.to(device) for k, v in encodings.items()}

        with torch.no_grad():
            outputs = model(**encodings)

        # Convert logits to probabilities
        probabilities = torch.sigmoid(outputs.logits).cpu().numpy()

        # Apply threshold to get predicted labels
        batch_predictions = mlb.inverse_transform(probabilities > threshold)
        all_predictions.extend(batch_predictions)

        del encodings  # Free memory
        torch.cuda.empty_cache()  # Clear CUDA memory

    return all_predictions

# Generate predictions for unlabeled data
all_texts = df["combined_text"].tolist()
predicted_labels = predict_unlabeled_data(all_texts, mlb, batch_size=16)

# Assign predictions back to original dataset while preserving order
df["Predicted_Taxonomy_Labels"] = predicted_labels

# Save updated dataset
df.to_csv("predicted_labels.csv", index=False)
print("Predictions saved to predicted_labels.csv")

print(predicted_labels)
